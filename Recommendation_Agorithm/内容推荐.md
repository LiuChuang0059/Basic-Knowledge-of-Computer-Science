# 0. 用户画像

* 用户向量化 
> 物品数量非常大 需要一个召回阶段，预先筛选一部分物品，

* 向量的纬度 不是 标签 ，不一定需要人理解

* 用户画像的量化， 应该交给机器，并且以目标为导向， 以推荐效果的额好坏来反向优化用户画像

* 构建用户画像方法
  * 查户口： 注册资料，购买历史，阅读历史 ---- 针对用户冷启动
  * 堆数据： 从历史行为数据挖掘标签，标签纬度上做数据统计
  * 黑盒子： 机器学习： 学习无法直观理解的稠密向量
     * 潜语义模型
     * 矩阵分解得到隐藏因子
     * 深度学习用户的Embedding 向量



--------

# 1. 文本信息

## 1， 用户端-- User Profile

* 注册的姓名，个性签名
* 发表的动态，评论，日记
* 聊天记录

## 2. 物品端--- item Profile

* 物品的标题，描述
* 物品的本身内容（新闻）
* 其他的基本属性的文本

--------

# 2. 构建用户画像

* 文本结构化， 保留关键信息
> 准确性， 粒度 ， 覆盖面

*  根据用户行为数据，把物品的结构化结果传递给用户

## 1. 结构化文本

* 关键字提取： TF-IDF 和 TextRank
* 实体识别：人物、位置和地点、著作、影视剧、历史事件和热点事件...

* 内容分类：将文本按照分类体系分类，用分类来表达较粗粒度的结构信息

* 文本 ：在无人制定分类体系的前提下，无监督地将文本划分成多个类，类编号也是用户画像的常见构成

* 主题模型： 从大量的文本中嘘唏主题向量，预测新的文本的主题概率分布，

* 嵌入： Embedding： 挖掘字面意思之下的语义信息

--------

# 3.  文本结构化算法

## 1. TF-IDF （Term Frequency   INverse Document Frequency）

> * 一篇文字中 反复出现的词会更重要
> * 在所有文本中都出现的词 不重要

* TF : 词频-- 出现的次数
* IDF : 统计每一个词出现在了多少个文本中，记为n（文档频率） 总的文档数目N----$$IDF = log（N/n+1）$$
  * 出现的文档数目少， IDF大
  * + 1 防止新词的0 出现无穷大
  * 对于新词，可以赋值为0 ，或者默认赋值为所有词的平均文档频率

* 计算 TR ✖️ IDF = 词的权重
  * 计算所有词权重的平均值， 去在权重平均值之上的词作为关键词
  
 
## 2. TextRank

算法思想：

* 1. 文本中，设定窗口宽度； k个词，统计窗口内的词和词的共现关系-- 看成无向图
* 2. 所有词初始化的重要性都是 1；
* 3. 每个节点把自己的权重平均分配给“和自己有连接“的其他节点
* 4. 每个节点将所有其他节点分给自己的权重求和，作为自己的新权重；
* 5. 如此反复迭代第 3、4 两步，直到所有的节点权重收敛为止。

那些有共现关系的会互相支持对方成为关键词。




## 3. 内容分类

* 门户网站具有自己的频道体系，内容分类体系，

* 短文本分类的方法是 SVM 常用工具-- Facebook的开源FastText


## 4. 实体识别 - Named- Entity Recognition
> 序列标注问题： 给定字符序列，对每一个字符分类

* 1. 词性标注：对每一个分好的词，分类为定义的词性集合的之一

* 2.实体识别：对每一个分好的词，识别为定义的命名实体集合之一
* 3. 分词问题：对每一个字符分类为“词开始”“词中间”“词结束”三...

常用的算法 ： 隐形马尔可夫模型HMM 条件随机场 CRF

常用工具： sapCy > NLTK


## 5. 聚类分析

主题模型：LDA的主题模型得到更好的软聚类的效果，一条文本属于多个类簇。

* LDA 模型需要设定主题个数，如果你有时间，那么这个 K 可通过实验进行对比挑选：每次计算K个主题俩俩之间的平均相似度，选择一个较低的K值？？
* 计算资源足够，主题数目可以多一些 


开源的LDA训练工具有 Gensim PLDA 


## 6. 词嵌入 ---得到一个 稠密的向量

Word2Vec ： 浅层神经网络学习得到每个词的向量表达



-----

# 4. 标签选择

* 把用户对物品的行为，消费或者没有消费看成是一个分类问题，用来标注若干数据

* 基本思想

  * 物品的结构化内容 看成文档
  * 用户对物品的行为看成是类别
  * 每个用户看见过的物品就是一个文本集合
  * 在这个文本集合上使用集合上使用特征选择算法选出每个用户关心的东西

## 1. 卡方检验 CHI -- 特征选择算法

* CHI 是有监督的 ，需要提供分类标注信息
> 因为文本分类任务中，挑选关键词就是为了分类任务，而不是挑选出一种直观上看着重要的词

* 卡方检验本质： ”词和某个类别C相互独立“的这个假设是否成立，假设偏离越多，说明这个词和类别C  暗中有联系

* 卡方值计算

<div align="center"> <img src="https://github.com/LiuChuang0059/ComplexNetwork-DataMining/blob/master/Recommendation_Agorithm/Image/%E5%8D%A1%E6%96%B9%E6%A3%80%E9%AA%8C.png" width="400"/> </div><br>

## 2. 信息增益（IG）--有监督的关键词选择方法

信息熵： 一批文本被标注了类别

* 各个类别的文本数量差不多的时候，信息熵比较大
* 少数类别的文本数量明显较多的时候，信息熵就比较小

信息增益计算

* 统计全局文本的信息熵
* 统计每个词的条件熵，知道了一个词之后，再统计文本的信息熵，分别计算包含词和不包含词的两部分信息熵

* 两者相减就是每个词的信息增益

----


# 5. 内容推荐

推荐系统需要接入新的物品，没有用户反馈，需要内容推荐


* 1.  抓： 抓取数据 补充内容源
* 2.  洗： 抓来的数据进行清洗
* 3.  挖： 对内容进行深入挖掘
* 4.  算： 匹配用户的兴趣和物品的属性，计算更合理的相关性


基于内容推荐的框架图



<div align="center"> <img src="https://github.com/LiuChuang0059/ComplexNetwork-DataMining/blob/master/Recommendation_Agorithm/Image/%E5%86%85%E5%AE%B9%E6%8E%A8%E8%8D%90%E7%9A%84%E6%A1%86%E6%9E%B6%E5%9B%BE.png" width="600"/> </div><br>

* 没有给用户推荐过的新内容，经过相同的内容分析过程，经过推荐算法匹配，计算得到新的推荐列表推荐给用户


### 1. 内容源

爬虫技术

### 2. 内容分析和用户分析

* 随着内容分析的深入，抓住的用户群体更加细致

* 内容分析的 产出
 * 结构化的内容库
 * 内容分析模型
 
* 内容分析模型
 * 分类器模型
 * 主题模型
 * 实体识别模型
 * 嵌入模型
 
> 新的物品加入的时候，需要实时的推荐出去，对内容进行实时分析，提取结构化的内容，用于用户画像的匹配



### 3. 内容推荐系统

* 用户饿的画像用户表示为稀疏的向量，内容端也有对应的稀疏向量，计算余弦相似性，根据相似度进行推荐

**信息检索中的相关性计算方法来做推荐匹配计算： BM25F算法---Lucene实现**
 
 
* 机器学习的方法
> 收集样本，训练预估模型
  * 样本有两部分构成： 一部分是特征，包含用户端的画像内容，物品端的结构化内容，上下文场景信息（时间，地理位置，设备）
  * 另一部分就是 用户行为，作为标注信息，包含 有反馈和无反馈




-------------




<div align="center"> <img src="https://github.com/LiuChuang0059/ComplexNetwork-DataMining/blob/master/Recommendation_Agorithm/Image/%E5%86%85%E5%AE%B9%E6%8E%A8%E8%8D%90%E6%80%BB%E7%BB%93.jpg" width="600"/> </div><br>






















































































