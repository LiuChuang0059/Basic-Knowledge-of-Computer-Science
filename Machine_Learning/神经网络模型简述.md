---

* 原文 link ---https://towardsdatascience.com/the-mostly-complete-chart-of-neural-networks-explained-3fb6f2367464

* 本文主要工作为 翻译 + 少许补充

-----

![](https://github.com/LiuChuang0059/large_file/blob/master/pic/0yjk6.jpg)

---


### 1. 感知机 -- Perception

![](https://github.com/LiuChuang0059/large_file/blob/master/pic/fzg0m.jpg)


* 最原始最简单的神经元模型
* 输入 -> 经过一些运算 -> 输出

> 简单应用： For instance, we'd have x1=1 if the weather is good, and x1=0 if the weather is bad. Similarly, x2=1 if your boyfriend or girlfriend wants to go, and x2=0 if not. And similarly again for x3 and public transit

----


### 2. 前馈神经网络 -- Feed Forword


![](https://github.com/LiuChuang0059/large_file/blob/master/pic/5y1qv.jpg)


#### 1. 特征

* 所有的节点都是全连接的，也可以称为全连接神经网络（Fully Connected Neural Network，
FCNN）

* 整个网络中的信息是朝一个方向传播，没有反向的信息传播，可以用一
个有向无环路图表示
> 注意与 梯度下降反向传播 计算的 区别


* 输入层和输出层之间有隐藏层 ；所以前馈神经网络也经常称为多层感知器（Multi-Layer Perceptron，MLP）--
> 叫多层感知机不太合理。前馈神经网络其实是由多层的logistic
回归模型（连续的非线性函数）组成，而不是由多层的感知器（不连续的非线
性函数）组成


* 大多数情况下，这一类的网络使用 **反向传播算法** 训练


#### 2. 应用

**前馈神经网络具有很强的拟合能力，常见的连续非线性函数都可以用前馈
神经网络来近似。---- 通用近似定理**

----


### 3. 径向基函数神经网络  -- Radial Basis Network


> 实际上就是 前馈神经网络， 只不过激活函数使用的是 径向基函数，而不是 Logistics 函数

* logistics 函数，能够将任意的数值转换到 0-1 之间；这有利于分类和决策的问题，但是不太适合连续数值的问题。

* 相反，径向基函数 能够告诉我们离目标距离多远，，这很有利于 函数近似逼近

---



### 4. 深度前馈神经网络 -- Deep Feed Forward

> 实际上就是前馈神经网络，只不过隐藏层的数目 大于 1

* DFF 与 FF 最大的区别，在于计算反向传播的时候，由于层数多了，反向传播计算次数成指数式增长

* 在 2000 年左右，人们 develop 一些新的方法让 DFF 高效训练， DFF 才得以实际应用


---

### 5. 循环神经网络 -- RNN
> 详述可以参照我的另一篇笔记 [link](https://github.com/LiuChuang0059/ComplexNetwork-DataMining/blob/master/techs/RNN/RNN%E6%A6%82%E8%BF%B0.md)


![](https://github.com/LiuChuang0059/large_file/blob/master/pic/15xxz.jpg)

* 循环神经网络通过使用带自反馈的神经元，能够处理任意长度的序列数据的神经网络

* FNN:模拟任何函数 RNN:模拟任何程序(计算过程)


#### 5-1. 长短期记忆  -- LSTM
> 详述可以参考我的另一篇笔记 [link](https://github.com/LiuChuang0059/ComplexNetwork-DataMining/blob/master/techs/RNN/RNN%E6%A6%82%E8%BF%B0.md#1-lstm)

> LSTM 代码实战可以参考我的另一篇笔记 [link](https://github.com/LiuChuang0059/ComplexNetwork-DataMining/blob/master/techs/RNN/LSTM.md)


* 引入自循环的巧妙构思，以产生梯度长时间持续流动的路径
> LSTM 循 环网络除了外部的 RNN 循环外，还具有内部的 “LSTM 细胞’’ 循环(自环)


* 使自循环的权重视上下文而定，而不是固定的
> 最重要的组成部分是状态单元 s(t)，类似于渗透单元的线性自环，。然而，此处自环的权重(或相关联的时间常数)由 遗忘门 (forget gate) f(t) 控制




#### 5-2 门控循环单元 --- GRU

> 类似于 LSTM

* 相比于 LSTM ，门的数目少， 参数少，模型简单，现在用的越来越多


---

### 6. 自动编码机 -- Auto Encoder
> 非监督模型

![](https://github.com/LiuChuang0059/large_file/blob/master/pic/jxy0i.jpg)

* 自编码器是两层的神经网络，输入层到隐 藏层用来编码，隐藏层到输出层用来解码


* 自编码器训练使得 输入和输出尽可能 的相同， 就可以使用中间的隐藏层（低维度）代表 输入（高维度）

**常用于特征压缩，分类等问题**

----


### 7.  变分自动编码机 --  Variational Auto Encoder


* 相比于 自动编码器，VAE 压缩的是概率，而不是特征

* AE 能够做到的事 是 归纳概括数据

* VAE 做的 两个时间的相关性

> 暂不太懂，留坑

----

### 8. 去噪自编码器 -- Denoising AE

* 为了迫使隐藏层单元发现更多鲁棒性好的特征, 以及阻止它学习恒等函数, 我们拿受损的输入来训练自编码器重构输入.

* DAE 在输入上加入一些噪声，随机噪声，重建无噪声的输入。

----

### 9.   稀疏自动编码机--- Sparse AE


* 结构和 AE 类似，但是 隐藏层神经元数目比输入层多

* 某些情况能偶反应数据的隐藏群组特征


---


### 10. 马尔可夫链  -- Markov Chain

* 利用转移概率矩阵解决问题

* 状态只取决于前一个状态


---


### 11.    -- Hopfield Network

> 反馈性神经网络

![](https://github.com/LiuChuang0059/large_file/blob/master/pic/b05zd.jpg)

* Hopfield神经网络是一种单层互相全连接的反馈型神经网络。每个神经元既是输入也是输出，

* 网络中的神经元在t时刻的输出状态实际上间接地与自己t-1时刻的输出状态有关。神经元之间互连接，所以得到的权重矩阵将是对称矩阵。

*  Hopfield 神经网络成功引入能量函数的概念，当网络达到稳定状态的时候，也就是它的能量函数达到最小的时候。


#### 2.

* Hopfield 神经网络模型有离散型和连续性两种，离散型适用于联想记忆，连续性适合处理优化问题。

* 例如 输入半个 序列或者图片， 返回整个样本

----

### 13. 玻尔兹曼机  Boltzmann Machine
![](https://github.com/LiuChuang0059/large_file/blob/master/pic/0jxgq.jpg)

* 类似于 HN 网络 ；一部分输入神经元，一部分 隐藏神经元

* 当隐藏神经元更新状态的时候， 输入神经元变为输出神经元
>  神经元一个一个的更新


* 多个 玻尔兹曼机堆叠起来就构成了深度置信网络


----

### 14. 受限玻尔兹曼机 -- RBM

#### 1.

![](https://github.com/LiuChuang0059/large_file/blob/master/pic/mz1ov.jpg)



* 相邻层之间是相连的，但是同层之间的节点是不相连的。不存在层内通信，这就是 RBM 中的限制所在



* 每个可见节点负责处理网络需要学习的数据集中一个项目的一种低层次特征。
> 举例来说，如果处理的是一个灰度图像的数据集，则每个可见节点将接收一张图像中每个像素的像素值。（MNIST图像有784个像素，所以处理这类图像的神经网络的一个可见层必须有784个输入节点。）



#### 2 重构

![](https://github.com/LiuChuang0059/large_file/blob/master/pic/u94dp.jpg)

* 受限玻尔兹曼机如何在无监督情况下学习重构数据（无监督指测试数据集没有作为实际基准的标签），在可见层和第一隐藏层之间进行多次正向和反向传递，而无需加大网络的深度。

* 第一隐藏层的激活值成为反向传递中的输入。这些输入值与同样的权重相乘，每两个相连的节点之间各有一个权重，就像正向传递中输入x的加权运算一样。这些乘积的和再与每个可见层的偏差相加，所得结果就是重构值，亦即原始输入的近似值


------

### 15. 深度置信网络 --  Deep Belief Network DBN


![](https://github.com/LiuChuang0059/large_file/blob/master/pic/cbpu8.jpg)

* 受限玻尔兹曼的堆叠，

* DBNs是一个概率生成模型，利用已经学习到的模式，生成数据


----


### 16. 卷积神经网络 -- CNN
>  详细内容可以参考我的另一篇笔记 [link](https://liuchuang0059.github.io/2018/12/22/CNN%E7%AE%80%E8%BF%B0/)


---

### 17  GAN

---


















# 参考

1. 前馈神经网络 --- https://nndl.github.io/chap-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.pdf

2. 径向基函数（RBF）神经网络 --- http://www.cnblogs.com/zhangchaoyang/articles/2591663.html

3. VAE -- https://zhuanlan.zhihu.com/p/21741426

4. 去噪自编码器 -- https://blog.csdn.net/enjoyyl/article/details/44629073#%E5%8E%BB%E5%99%AA%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8denoising-autoencoders

5. Hopfiled 神经网络 -- https://blog.csdn.net/weixin_39707121/article/details/79041536

6. 受限玻尔兹曼机 --- https://deeplearning4j.org/cn/restrictedboltzmannmachine.html


7. 深度置信网络 -- https://blog.csdn.net/win_in_action/article/details/25333671

8. Understanding Convolutions ----- http://colah.github.io/posts/2014-07-Understanding-Convolutions/













