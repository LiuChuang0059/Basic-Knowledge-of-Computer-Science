
![](https://github.com/LiuChuang0059/large_file/blob/master/pic/1hc35.jpg)

----





# 1. 粒子群算法简介



![](https://github.com/LiuChuang0059/large_file/blob/master/pic/nuapo.jpg)





* 粒子群算法（ Particle Swarm Optimization, PSO）最早是由Eberhart和Kennedy于1995年提出，它的基本概念源于对鸟群觅食行为的研究。设想这样一个场景:一群鸟在随机搜寻食物，在这个区域里只有一块食物，所有的鸟都不知道食物在哪里，但是它们知道当前的位置离食物还有多远。那么找到食物的最优策略是什么呢?最简单有效的就是搜寻目前离食物最近的鸟的周围区域。

* PSO算法就从这种生物种群行为特性中得到启发并用于求解优化问题。在PSO中，每个优化问题的潜在解都可以想象成d维搜索空间上的一个点，我们称之为“粒子”（Particle），所有的粒子都有一个被目标函数决定的适应值(Fitness Value )，每个粒子还有一个速度决定他们飞翔的方向和距离，然后粒子们就追随当前的最优粒子在解空间中搜索。

![](https://github.com/LiuChuang0059/large_file/blob/master/pic/20u2y.gif)

   图片来源于 知乎 [数学建模andMATLAB](https://www.zhihu.com/people/shu-xue-jian-mo-andmatlab)

# 2. 粒子群算法简化

### 1. Basic PSO

速度更新公式和位置更新公式

![](https://github.com/LiuChuang0059/large_file/blob/master/pic/58z4z.jpg)

其中:i=1,2,…,m;d=1,2,…,D;r 1 和 r 2 是服从 U(0,1)分布的随机数;学习因子 c 1 和 c 2 为非负常数,通常取 c 1 =c 2 =2; v  ∈[−v max ,v max ],v max 是由用户设定的常数.迭代终止条件为预设的最大迭代次数或(和)预定的最小适应度阈值.动量惯性系数ω来提高跳出局部极值能力，控制速度的约束因子α。

### 2. Simple PSO

经过参考 6 文献的证明，,约束粒子速度范围v  ∈[−v max ,v max ]与添加约束因子 α是等价的。基于此文献7 详细说明，粒子速度 v i 代表粒子移动的快慢程度,粒子移动速度的大小并不代表粒子能够有效 趋近最优解位置,反而可能造成粒子偏离正确的进化方向,出现粒子“发散”现象。

通过简单的迭代计算，basic PSO 的更新公式可以推出，

![](https://github.com/LiuChuang0059/large_file/blob/master/pic/625cu.jpg)



文献 7 中详细的分析得出，**sPSO 比 bPSO 在收敛精度和收敛速度上有非常显著的提高**





# 3. 粒子群算法简化 --  Code

### 根据上面的流程和更新公式，写代码



```python
class PSO(RNN_train):
    def __init__(self, pN, dim, max_iter):
        self.w = 0.8
        self.c1 = 2
        self.c2 = 2
        self.space = [0,10]  # 粒子搜索范围
        self.pN = pN  #粒子数量
        self.dim = dim  #搜索维度
        self.max_iter = max_iter  #迭代次数
        self.X = np.zeros((self.pN, self.dim))  #所有粒子的位置和速度
        self.V = np.zeros((self.pN, self.dim))
        self.pbest = np.zeros((self.pN, self.dim))  #个体经历的最佳位置和全局最佳位置
        self.gbest = np.zeros((1, self.dim))
        self.p_fit = np.zeros(self.pN)  #每个个体的历史最佳适应值
        self.fit = 1e10     #全局最佳适应值
        self.loss50 = 1e10


    def init_Population(self):
        #初始化种群

        for i in range(self.pN):
            for j in range(self.dim):
                self.X[i][j] = random.uniform(self.space[0], self.space[1])
                #self.V[i][j] = random.uniform(self.space[0], self.space[1])
            print("Point %d 初始化坐标 ： "%i)
            print(self.X[i]) #  打印初始点坐标
            self.pbest[i] = self.X[i]
            RNN = RNN_train(self.X[i][0], self.X[i][1])
            history, model = RNN.model_train()
            RNN.train_plot(history)
            peak_shift_MSE, test_loss_total_ave, peak_half_loss_ave, peak_twenty_percent_loss_ave = RNN.loss_cal(
                model)
            self.p_fit[i] = peak_shift_MSE
            if (peak_shift_MSE < self.fit):
                self.fit = peak_shift_MSE
                self.gbest = self.X[i]
                self.loss50 = peak_half_loss_ave
            print(" 初始化中 全局最佳点的坐标： ")
            print(self.gbest)
            print("---------\n")

    def iterator(self):
        #更新粒子位置----------------------------------
        print("--------------------------------------\n")
        print("--------------------------------------\n")
        print("更新粒子位置")
        fitness = []
        best_pos = []
        loss50_list = []
        for t in range(self.max_iter):
            print("It is the %d iteration "%t)
            for i in range(self.pN):
                self.X[i] = self.w*self.X[i] + self.c1*random.random()*(self.pbest[i] - self.X[i]) + \
                            self.c2*random.random()*(self.gbest - self.X[i])
                print("Iterator : %d , Ponit : %d"%(t,i))
                print(self.X[i])
                print("--------------------------------------\n")
                RNN = RNN_train(self.X[i][0], self.X[i][1])
                history, model = RNN.model_train()
                RNN.train_plot(history)
                peak_shift_MSE, test_loss_total_ave, peak_half_loss_ave, peak_twenty_percent_loss_ave = RNN.loss_cal(
                    model)
                temp = peak_shift_MSE
                if (temp < self.p_fit[i]):  #更新个体最优
                    self.p_fit[i] = temp
                    self.pbest[i] = self.X[i]
                    if (self.p_fit[i] < self.fit):  #更新全局最优
                        self.gbest = self.X[i]
                        self.fit = self.p_fit[i]
                        self.loss50 = peak_half_loss_ave
            fitness.append(self.fit)
            best_pos.append(self.gbest)
            loss50_list.append(self.loss50)
            print("-------------------\n")
            print("第 %d 个循环结果： "%t)
            print("全局最优点坐标 :")
            print(self.gbest)
            print("MSE: %f"%self.fit)  #输出最优值
            print(self.loss50)
            print("--------------------------------------\n")
            print("--------------------------------------\n")
        return fitness,MSE,best_pos
if __name__ == '__main__':
    my_pso = PSO(pN=5,dim=2,max_iter=5)
    my_pso.init_Population()
    fitness,best_pos = my_pso.iterator()
```



###  结果

![](https://github.com/LiuChuang0059/large_file/blob/master/pic/k1iap.jpg)





# 4. 其他



1. 选好适应度函数，以及各个参数的范围
2. PSO 有很多很多的扩展，按需了解学习




# 参考

1. <https://www.zhihu.com/question/23103725>

2. <https://blog.csdn.net/niuyongjie/article/details/1569671>

3. http://www.zihaochang.com/2018/02/02/%E7%B2%92%E5%AD%90%E7%BE%A4%E7%AE%97%E6%B3%95-PSO-Python-%E5%AE%9E%E7%8E%B0/

4. https://blog.csdn.net/ztf312/article/details/75669685

5. https://github.com/tisimst/pyswarm

6. Clerc M. The swarm and the queen: Towards a deterministic and adaptive particle swarm optimization. In: Proc. of the ICEC.

   Washington, 1999. 1951−1957.

7. 胡 旺+, 李志蜀，一种更简化而高效的粒子群优化算法∗，软件学报，2007




