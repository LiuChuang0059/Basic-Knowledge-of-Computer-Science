原文 ： 1. [The 5 Feature Selection Algorithms every Data Scientist should know]

link : https://towardsdatascience.com/the-5-feature-selection-algorithms-every-data-scientist-need-to-know-3a6b566efd2

2. Feature Selection For Machine Learning in Python

link : https://machinelearningmastery.com/feature-selection-machine-learning-python/








----


## 1. 为什么进行特征选择

为什么我们不能将所有的特征都输入到 机器学习算法中， 让机器来决定哪种特征重要。

1. 纬度灾难 ： 过拟合

当数据的特征纬度增加的时候，  特征空间的维度逐渐增加，，特征空间的样本密度成指数下降，特征空间内的数据越来越稀疏。由于稀疏性，我们更加容易找到一个超平面来实现分类。但是使用太多特征导致过拟合。模型学习了过多样本数据的异常特征（噪声）， 而对新数据的泛化能力不好。

如果增加特征维度，为了覆盖同样的特征值范围、防止过拟合，那么所需的训练样本数量就会成指数型增长。

> 假设我们只使用一个特征来训练分类器，1D特征值的范围限定在0到1之间，且每只猫和狗对应的特征值是唯一的。如果我们希望训练样本的特征值占特征值范围的20%，那么训练样本的数量就要达到总体样本数的20%。现在，如果增加第二个特征，也就是从直线变为平面2D特征空间，这种情况下，如果要覆盖特征值范围的20%，那么训练样本数量就要达到总体样本数的45%（0.45 *0.45=0.2）。而在3D空间中，如果要覆盖特征值范围的20%，就需要训练样本数量达到总体样本数的58%（0.58 * 0.58 * 0.58=0.2)


![](https://i.loli.net/2019/08/06/QxY8JWaCP4rvbEK.jpg)


落在单位圆之外的训练样本位于特征空间角落处，比位于特征空间中心处的样本更难进行分类。
如下图所示， 增加特征空间的维度，超立方体的体积都是1，而半径为0.5的超球体的体积随着维度d的变化逐渐趋近于0
，说明高维空间中大部分数据分布在角落处。

![](https://i.loli.net/2019/08/06/aHUw7Oi3V4ATg9P.jpg)


2. 奥卡姆剃刀

模型需要简单和可解释性。 当我们加入过多的特征，模型失去了可解释的能力


3. 垃圾的输入，获得垃圾的输出

大多数的时候，我们会有很多没有 没有意义（不包含什么信息）的特征， 例如，姓名和 ID 。这样的输入会产出一个低质量的输出


**好处和优势**

1.  减小 过拟合

2. 提高准确率

3. 减少训练时间

------



## 2. 如何筛选有用的特征

特征筛选的方法主要有 3 类：

* 基于过滤 ： 指定一些度量标准， 然后给予次进行过滤特征。

* 基于 Wrapper ： 将一组特征的选择看作一个搜索的问题

* Embedded：  使用内置的特征选择方法


-----


## 3. Pearson Correlation coefficient （PCC）

皮尔逊相关系数 ： 计算两个变量之间的关联性。

值在 [-1, 1] 之间， 1 代表完全正线性相关， 0 代表没有线性关联。


![](https://i.loli.net/2019/08/06/w1Tez6Al2nEv73F.png)

检查 目标值和 数据集中特征之间的 PCC 的绝对值， 根据这个标准保留前 n 的特征。


```python
def pcc_selector(X,y, num_feat):
	cor_list = []
	# 将特征 转换为列表
	feature_name = X.columns.tolist()
	# 计算每个特征和 y 之间的关联性
	for i in feature_name:
		cor = np.corrcoef(X[i], y)[0,1] # 计算结果是一个对称的 2维的矩阵，
        #    x   y

		# x  1

		# y      1

		# 	我们只需要 取 [0,1] 的 值

		cor_list.append(cor)

	# 一些计算值是 NaN 我们用 0 值替换

	cor_list = [0 if np.isnan(i) else i for i in cor_list]
	# 根据 计算的值筛选特征
	cor_feature = X.iloc[:,np.argsort(np.abs(cor_list))[-num_feats:]].columns.tolist()

	# np.argsort 返回排序的 切片指数
	# >>> x = np.array([3, 1, 2])
	#>>> np.argsort(x)
	#array([1, 2, 0])

	cor_support =[True if i in cor_feature else False for i in feature_name]

	return cor_support, cor_support

```

----

## 4. Chi-Squared

卡方检验 ： 卡方检验用于确定一个或多个类别的预期频率与观察频率之间是否存在显著差异。 判断变量是不是独立的

![](https://i.loli.net/2019/08/06/s6Lqx8Zja5bH9YD.jpg)


```python
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2
from sklearn.preprocessing import MinMaxScaler
X_norm = MinMaxScaler().fit_transform(X)
chi_selector = SelectKBest(chi2, k=num_feats)
chi_selector.fit(X_norm, y)
chi_support = chi_selector.get_support()
chi_feature = X.loc[:,chi_support].columns.tolist()
print(str(len(chi_feature)), 'selected features')

```

---

## 5. Recursive Feature Elimination 递归性特征消除

Wrapper based

> 通过学习器返回的 coef_ 属性 或者 feature_importances_ 属性来获得每个特征的重要程度。 然后，从当前的特征集合中移除最不重要的特征。在特征集合上不断的重复递归这个步骤，直到最终达到所需要的特征数量为

![](https://i.loli.net/2019/08/06/lgTsznEeZjQ2IH7.jpg)

我们需要选择一个学习器 estimator, 选择的逻辑回归

```python

from sklearn.feature_selection import RFE
from sklearn.linear_model import LogisticRegression
rfe_selector = RFE(estimator=LogisticRegression(), n_features_to_select=num_feats, step=10, verbose=5)
rfe_selector.fit(X_norm, y)
rfe_support = rfe_selector.get_support()
rfe_feature = X.loc[:,rfe_support].columns.tolist()
print(str(len(rfe_feature)), 'selected features')


```


-----

## 6. Lasso   least absolute shrinkage and selection operator

1. 稀疏性假设 ： 考虑一个线性回归模型，有一个因变量Y，但有成百上千的自变量X。我们假设，只有有限个X的回归系数不为0，但其余的都是0。也就是说他们跟 Y 没有显著关联。找到重要的 X ，便于我们理解数据

2. Lasso 适用于高维变量和稀疏性假设情况。Lasso相比于普通最小二乘估计，可以在变量众多的时候快速有效地提取出重要变量，简化模型。

3.  MSE由偏差和方差两部分组成，Lasso虽然是有偏估计，但是在引入一定的偏差的同时，可能可以大量降低估计的方差，从而降低整体的MSE

4. Lasso Regularizer  强迫一些特征的权重为 0 ， 筛选特征

```python

from sklearn.feature_selection import SelectFromModel
from sklearn.linear_model import LogisticRegression

embeded_lr_selector = SelectFromModel(LogisticRegression(penalty="l1"), max_features=num_feats)
embeded_lr_selector.fit(X_norm, y)

embeded_lr_support = embeded_lr_selector.get_support()
embeded_lr_feature = X.loc[:,embeded_lr_support].columns.tolist()
print(str(len(embeded_lr_feature)), 'selected features')


```


## 7. 基于 树的 ： RandomForest   LightGBM Or an XGBoost


```python

from sklearn.feature_selection import SelectFromModel
from sklearn.ensemble import RandomForestClassifier

embeded_rf_selector = SelectFromModel(RandomForestClassifier(n_estimators=100), max_features=num_feats)
embeded_rf_selector.fit(X, y)

embeded_rf_support = embeded_rf_selector.get_support()
embeded_rf_feature = X.loc[:,embeded_rf_support].columns.tolist()
print(str(len(embeded_rf_feature)), 'selected features')


from sklearn.feature_selection import SelectFromModel
from lightgbm import LGBMClassifier

lgbc=LGBMClassifier(n_estimators=500, learning_rate=0.05, num_leaves=32, colsample_bytree=0.2,
            reg_alpha=3, reg_lambda=1, min_split_gain=0.01, min_child_weight=40)

embeded_lgb_selector = SelectFromModel(lgbc, max_features=num_feats)
embeded_lgb_selector.fit(X, y)

embeded_lgb_support = embeded_lgb_selector.get_support()
embeded_lgb_feature = X.loc[:,embeded_lgb_support].columns.tolist()
print(str(len(embeded_lgb_feature)), 'selected features')

```

## 8.  Principal Component Analysis  PCA

PCA 使用线性代数将数据集转换成为一个压缩的形式，通常 PCA 被叫做数据降维技术，




## 9. 全部使用



如果可以， 合并所有的特征


```python

# put all selection together
feature_selection_df = pd.DataFrame({'Feature':feature_name, 'Pearson':cor_support, 'Chi-2':chi_support, 'RFE':rfe_support, 'Logistics':embeded_lr_support,
                                    'Random Forest':embeded_rf_support, 'LightGBM':embeded_lgb_support})
# count the selected times for each feature
feature_selection_df['Total'] = np.sum(feature_selection_df, axis=1)
# display the top 100
feature_selection_df = feature_selection_df.sort_values(['Total','Feature'] , ascending=False)
feature_selection_df.index = range(1, len(feature_selection_df)+1)
feature_selection_df.head(num_feats)

```


![](https://i.loli.net/2019/08/06/MimCFaAybw7cf1J.png)



## 10. 对于时间序列的特征

详见 参考8

对于多种时间窗口， 为了评价那种时间窗口效果较好， 先进行简单的 logistic 回归， 分析回归系数。 回归系数和相应的特征有着关联。

> To evaluate the feature selection, I performed a logistic regression and analyzed the regression coefficients. The logistic regression coefficients correspond to the change in log odds of the associated feature meaning the logarithm of how the odds (ratio of the probability of a crash vs no crash) change with a change in that feature when all other features are being held constant. For the plot below I transformed the log odds to odds. Odds greater than 1 indicate that the crash probability increases with an increase in the corresponding feature.

![](https://i.loli.net/2019/08/28/rQNDTy36evhXYJi.png)

odds ratio 大于 1 意味着相应的概率随着相应特征的增加而增加。





# 参考
----

1. 机器学习中的维度灾难 --- https://zhuanlan.zhihu.com/p/26945814

2. https://en.wikipedia.org/wiki/Pearson_correlation_coefficient

3. 卡方检验 ： https://www.shuxuele.com/data/chi-square-test.html

4. 递归式特征消除：Recursive feature elimination : https://blog.csdn.net/FontThrone/article/details/79004874

5. 递归式特征消除. 《机器学习系统设计》

6. https://scikit-learn.org/stable/modules/feature_selection.html#rfe

7. 统计学习：变量选择之Lasso : https://zhuanlan.zhihu.com/p/42122611


8. [评估选择的特征](https://towardsdatascience.com/predicting-stock-market-crashes-with-statistical-machine-learning-techniques-and-neural-networks-bb66bc3e3ccd)
