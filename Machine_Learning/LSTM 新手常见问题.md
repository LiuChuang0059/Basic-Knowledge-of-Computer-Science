
![](https://github.com/LiuChuang0059/large_file/blob/master/pic/cfq6c.jpg)

-----

### 0. RNN ç®€å•ç†è§£

![](https://github.com/LiuChuang0059/large_file/blob/master/pic/ol2ns.jpg)

ä¸ºäº†é¢„æµ‹æœ€åçš„ç»“æœï¼Œæˆ‘å…ˆç”¨ç¬¬ä¸€ä¸ªè¯é¢„æµ‹ï¼Œå½“ç„¶ï¼Œåªç”¨ç¬¬ä¸€ä¸ªé¢„æµ‹çš„é¢„æµ‹ç»“æœè‚¯å®šä¸ç²¾ç¡®ï¼Œæˆ‘æŠŠè¿™ä¸ªç»“æœä½œä¸ºç‰¹å¾ï¼Œè·Ÿç¬¬äºŒè¯ä¸€èµ·ï¼Œæ¥é¢„æµ‹ç»“æœï¼›æ¥ç€ï¼Œæˆ‘ç”¨è¿™ä¸ªæ–°çš„é¢„æµ‹ç»“æœç»“åˆç¬¬ä¸‰è¯ï¼Œæ¥ä½œæ–°çš„é¢„æµ‹ï¼›ç„¶åé‡å¤è¿™ä¸ªè¿‡ç¨‹ï¼›ç›´åˆ°æœ€åä¸€ä¸ªè¯ã€‚è¿™æ ·ï¼Œå¦‚æœè¾“å…¥æœ‰nä¸ªè¯ï¼Œé‚£ä¹ˆæˆ‘ä»¬äº‹å®ä¸Šå¯¹ç»“æœä½œäº†næ¬¡é¢„æµ‹ï¼Œç»™å‡ºäº†nä¸ªé¢„æµ‹åºåˆ—ã€‚æ•´ä¸ªè¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹å…±äº«ä¸€ç»„å‚æ•°ã€‚å› æ­¤ï¼ŒRNNé™ä½äº†æ¨¡å‹çš„å‚æ•°æ•°ç›®ï¼Œé˜²æ­¢äº†è¿‡æ‹Ÿåˆï¼ŒåŒæ—¶ï¼Œå®ƒç”Ÿæ¥å°±æ˜¯ä¸ºå¤„ç†åºåˆ—é—®é¢˜è€Œè®¾è®¡çš„ï¼Œå› æ­¤ï¼Œç‰¹åˆ«é€‚åˆå¤„ç†åºåˆ—é—®é¢˜ã€‚

**è½¬è½½è‡ªè‹ç¥è‹å‰‘æ—https://spaces.ac.cn/archives/3924**



----
### 1. RNN è®°å¿†ä¿¡æ¯

ä¾‹å¦‚ç»™ä¸€ä¸ªå½±ç‰‡ä¸­çš„å›¾ç‰‡æ‰“æ ‡ç­¾ï¼ˆæ´»åŠ¨ï¼‰ï¼Œ å¦‚æœå‰ä¸€ä¸ªåœºæ™¯æ˜¯åœ¨æµ·æ»©ï¼Œ æˆ‘ä»¬åº”è¯¥åœ¨æ‰“æ ‡ç­¾çš„æ—¶å€™å°½å¯èƒ½çš„å€¾å‘æµ·æ»©æ´»åŠ¨ã€‚å¦‚æœå›¾ç‰‡é‡Œä¸€ä¸ªäººåœ¨æ°´ä¸­ï¼Œé‚£æˆ‘ä»¬æˆ‘ä»¬çŒœæµ‹ä»–çš„æ´»åŠ¨åº”è¯¥æ˜¯**æ¸¸æ³³**ï¼Œè€Œä¸æ˜¯æ´—æ¾¡ã€‚

æ‰€ä»¥æˆ‘ä»¬éœ€è¦è®©æˆ‘çš„æ¨¡å‹èƒ½å¤Ÿè¿½è¸ªçŠ¶æ€çš„å˜åŒ–

1. åœ¨çœ‹å®Œæ¯å¼ ç…§ç‰‡åï¼Œæ¨¡å‹ä¸ä»…è¦è¾“å‡ºä¸€ä¸ªæ ‡ç­¾ï¼Œè¿˜è¦**æ›´æ–°å­¦ä¹ åˆ°çš„çŸ¥è¯†**

2. åœ¨é‡åˆ°æ–°çš„ç…§ç‰‡çš„æ—¶å€™ï¼Œæ¨¡å‹éœ€è¦ç»“åˆä»–ä¹‹å‰è·å¾—åˆ°çš„ä¿¡æ¯æ¥æ›´å¥½çš„é¢„æµ‹

3. è¿™äº›å…³é”®çš„ä¸­é—´ä¿¡æ¯ï¼Œå·²ç»ç”±ç¥ç»ç½‘ç»œçš„éšè—å±‚ç¼–ç æå–å‡ºæ¥äº†ã€‚


![](https://github.com/LiuChuang0059/large_file/blob/master/pic/iq2sn.jpg)


----

### 2. LSTM çš„é•¿æœŸè®°å¿†

ä¸Šæ–‡ä¹‹ä»‹ç»äº†è®°å¿†å’Œæ›´æ–°ä¿¡æ¯ï¼Œä½†æ˜¯æ²¡æœ‰è€ƒè™‘å¦‚ä½•æ›´æ–°ï¼Œä½•æ—¶æ›´æ–°ï¼›æ‰€ä»¥ä¿¡æ¯æ›´æ–°çš„ä¼šå¾ˆæ··ä¹±ã€‚æ··ä¹±æ„å‘³ç€ä¿¡æ¯è¿…é€Ÿå˜åŒ–å’Œæ¶ˆå¤±ã€‚æ‰€ä»¥æˆ‘ä»¬éœ€è¦ç½‘ç»œä¿å­˜ä¸€ä¸ª**é•¿æœŸè®°å¿†**ï¼Œè€Œè®°å¿†çš„ä¿¡æ¯è¦ç¼“å’Œçš„æ”¹å˜ï¼ˆgentlyï¼‰

1. åŠ å…¥**é—å¿˜æœºåˆ¶** ï¼š ä¸€ä¸ªåœºæ™¯ç»“æŸäº†ï¼Œæ¨¡å‹å°±éœ€è¦å¿˜æ—§å½“å‰åœºæ™¯çš„ç›¸å…³ä¿¡æ¯ï¼Œä¾‹å¦‚ï¼š ä½ç½®ï¼Œæ—¶é—´ç­‰ã€‚ä½†æ˜¯**ä¸æ˜¯æ‰€æœ‰çš„é¢ä¿¡æ¯éƒ½é—å¿˜**ã€‚ä¾‹å¦‚ï¼Œè¿™ä¸€ä¸ªåœºæ™¯é‡Œé¢æœ‰äººæ­»äº†ï¼Œæ¨¡å‹å°±éœ€è¦è®°ä½è¿™ä¸ªä¿¡æ¯ã€‚

2. åŠ å…¥**ä¿å­˜æœºåˆ¶** ï¼š æ¨¡å‹çœ‹åˆ°ä¸€ä¸ªæ–°çš„ç…§ç‰‡ï¼Œéœ€è¦å­¦ä¹ å…³äºå›¾ç‰‡çš„ä¸€äº›ä¿¡æ¯æ˜¯å¦å€¼å¾—ä½¿ç”¨å’Œä¿å­˜

3. å½“ä¸€ä¸ªæ–°çš„è¾“å…¥è¿›æ¥çš„æ—¶å€™ï¼Œæ¨¡å‹å…ˆå¿˜è®°ä¸€äº›ä¸éœ€è¦è®°å¿†çš„ä¿¡æ¯ï¼Œç„¶åå†å˜˜å”ä¸€äº›æ–°çš„è¾“å…¥é‡Œé¢å€¼å¾—ä½¿ç”¨çš„ä¿¡æ¯ï¼ŒæŠŠä»–ä»¬ä¿å­˜åœ¨é•¿æœŸè®°å¿†é‡Œé¢ã€‚

4. **å°†é•¿æœŸè®°å¿†è½¬åŒ–ä¸ºå·¥ä½œè®°å¿†**ï¼š æœ€ç»ˆæ¨¡å‹éœ€è¦å­¦ä¹ å“ªéƒ¨åˆ†çš„é•¿æœŸè®°å¿†æ˜¯ç«‹å³å¯ç”¨çš„ã€‚
> ä¾‹å¦‚ æå„’çš„å¹´é¾„ä¿¡æ¯æ˜¯é•¿æœŸè®°å¿†é‡Œé¢ä¸€ä¸ªæœ‰ç”¨çš„ç‰‡æ®µï¼Œä½†æ˜¯å¦‚æœ æå„’ä¸åœ¨å½“å‰çš„åœºæ™¯é‡Œé¢ï¼Œè¿™ä¸ªä¿¡æ¯å¯èƒ½å°±æ— å…³ç´§è¦

----

### 3. LSTM è®°å¿†æœºåˆ¶ -- æ¶æ„
> è¯¦ç»†è¿ç®—å‚è€ƒ -- https://github.com/LiuChuang0059/ComplexNetwork-DataMining/blob/master/techs/RNN/RNN%E6%A6%82%E8%BF%B0.md#1-lstm

LSTM ä¼šè¾“å‡ºä¸¤ä¸ªçŠ¶æ€åˆ°ä¸‹ä¸€ä¸ªå•å…ƒï¼š cell state å’Œ hidden stateã€‚cell è´Ÿè´£å„ä¸ªéšè—çŠ¶æ€æˆ–è€…å‰é¢æ—¶é—´æ­¥çš„äº‹ä»¶ï¼Œè®°å¿†æ–¹å¼ä¸€èˆ¬é€šè¿‡ä¸‰ç§é—¨æ§æœºåˆ¶å®ç° ï¼š è¾“å…¥é—¨ï¼Œé—å¿˜é—¨ï¼Œå’Œè¾“å‡ºé—¨



#### 1. é—å¿˜é—¨

è´Ÿè´£ä»å•å…ƒçŠ¶æ€ä¸­ç§»é™¤ä¿¡æ¯ã€‚ä¾‹å¦‚ä¸‹å›¾ï¼Œå½“æ¨¡å‹é‡åˆ° person åé¢çš„ ç¬¬ä¸€ä¸ªå¥å·ï¼Œé—å¿˜é—¨å°±å¯èƒ½æ„è¯†åˆ°ä¸‹ä¸€ä¸ªè¯­å¥çš„è¯­å¢ƒå‘ç”Ÿå˜åŒ–ï¼Œå°±éœ€è¦é—å¿˜å‰ä¸€å¥çš„ä¸»è¯­ Bob

![](https://github.com/LiuChuang0059/large_file/blob/master/pic/huk6a.jpg)


#### 2. è¾“å…¥é—¨
 è¾“å…¥é—¨å°†ä¿¡æ¯æ·»åŠ åˆ°å•å…ƒçŠ¶æ€ï¼Œä¾‹å¦‚ä¸‹å›¾ï¼Œ

!![](https://github.com/LiuChuang0059/large_file/blob/master/pic/cwe7x.jpg)

* é€šè¿‡ sigmoid å‡½æ•°è°ƒèŠ‚éœ€è¦æ·»åŠ åˆ°å•å…ƒçŠ¶æ€çš„å€¼ï¼Œç±»ä¼¼äºä¸€ä¸ªæ»¤æ³¢å™¨è¿‡æ»¤æ¥è‡ª $h_{t-1}$ å’Œ $x_{t}$

* åˆ›å»ºä¸€ä¸ª åŒ…å«æ‰€æœ‰å¯èƒ½å€¼çš„å‘é‡ï¼Œæ·»åŠ åˆ°å•å…ƒçŠ¶æ€ä¸­ï¼Œä½¿ç”¨ tanh è¾“å‡º -1 åˆ° 1

* è°ƒèŠ‚æ»¤æ³¢å™¨çš„å€¼ä¹˜ä»¥åˆ›å»ºçš„å‘é‡


#### 3. è¾“å‡ºé—¨
è¾“å‡ºé—¨ä»å½“å‰çŠ¶æ€é€‰æ‹©æœ‰ç”¨çš„ä¿¡æ¯å¹¶å°†å…¶æ˜¾ç¤ºä¸ºè¾“å‡º

![](https://github.com/LiuChuang0059/large_file/blob/master/pic/5mddk.jpg)

* æŠŠ tanh å‡½æ•°åº”ç”¨åˆ°å•å…ƒçŠ¶æ€ä¹‹ååˆ›å»ºä¸€ä¸ªå‘é‡ï¼Œä»è€Œå°†å€¼ç¼©æ”¾åœ¨-1 åˆ°+1 ä¹‹é—´ã€‚

* $h_{t-1}$ å’Œ $x_{t}$ ç»è¿‡è¿‡æ»¤å™¨ï¼Œè°ƒèŠ‚éœ€è¦ä»ä¸Šè¿°åˆ›å»ºå‘é‡ä¸­è¾“å‡ºçš„å€¼

* è¿‡æ»¤å™¨å’Œ å‘é‡ä¹˜ ä½œä¸º**è¾“å‡º** å‘é€ï¼Œå¹¶å‘é€åˆ°ä¸‹ä¸€ä¸ªå•å…ƒçš„éšè—æ€

**è¿‡æ»¤å™¨å»ºç«‹åœ¨è¾“å…¥å’Œéšè—æ€ä¸Šï¼Œå¹¶åº”ç”¨åˆ°å•å…ƒçŠ¶æ€**






----


### 4. LSTM internals --- LSTM cell statesï¼Œotter memory mechanisms


> ä½¿ç”¨ LSTM è®­ç»ƒä¸€ä¸ªè®¡æ•°å™¨ï¼ŒaaaaaXbbbbbï¼› N ä¸ª a  é¢„æµ‹ Nä¸ª b

![](https://github.com/LiuChuang0059/large_file/blob/master/pic/6eq1o.jpg)

![](https://github.com/LiuChuang0059/large_file/blob/master/pic/zorpf.jpg)

* å¯¹æ¯”å‘ç°**å·¥ä½œè®°å¿†**çœ‹èµ·æ¥åƒé•¿æœŸè®°å¿†çš„ä¸€ä¸ª é”åŒ–ç‰ˆ
> å› ä¸º é•¿æœŸè®°å¿†è¢« tanh å‡½æ•°å‹ç¼©ï¼Œå¹¶ä¸”è¾“å‡ºé—¨ é™åˆ¶ä¸€éƒ¨åˆ†è¾“å‡ºã€‚

* è§‚å¯Ÿ cell state
> a çš„é¢œè‰²é€æ¸åŠ æ·±ç›´åˆ°é‡åˆ°é™åˆ¶ç¬¦ï¼Œç„¶åé¢œè‰²å†é€æ¸å˜æµ…


-----

### 5.  LSTM  çš„è®°å¿†çŠ¶æ€

åºåˆ—ï¼š
```
AxxxxxxYa
BxxxxxxYb
```

éœ€è¦æ¨¡å‹è®°å¿†å‰é¢çš„æ˜¯ Aã€è¿˜æ˜¯B

![](https://github.com/LiuChuang0059/large_file/blob/master/pic/dlzso.jpg)

* Neuron 8 æ˜¯ä¸€ä¸ª è®°å½• Açš„ç¥ç»å…ƒï¼Œ å¯ä»¥å‘ç° è¾“å…¥é—¨ å¿½ç•¥äº†å…¶ä¸­çš„å­—æ¯xï¼› è€Œéšè—çŠ¶æ€ä¼šè§¦å‘æ‰€æœ‰ä¸­é—´ x



ç»™å®šä¸‹é¢ä¸¤ä¸ªåºåˆ—ï¼Œè®­ç»ƒæ¨¡å‹ï¼Œç»™å®šä¸€ä¸ªå¦‚åˆï¼Œé¢„æµ‹ä¸‹ä¸€ä¸ªå€¼æ˜¯ä»€ä¹ˆã€‚

```
3, 0, 1, 2, 3
4, 0, 1, 2, 4

```

æ³¨æ„åˆ°ï¼Œä¸¤ä¸ªåºåˆ—ä¹‹é—´ä¿¡æ¯æœ‰å†²çªï¼Œ åœ¨åºåˆ—1é‡Œé¢ï¼Œç»™å®š 2ä¸ºè¾“å…¥ï¼Œéœ€è¦é¢„æµ‹çš„ç»“æœä¸º3ï¼Œ è€Œåœ¨åºåˆ—2 é‡Œé¢ï¼Œå¯»è¯è¾“å‡º 4
è¿™ä¸ªé—®é¢˜å¯¹äº**å¤šå±‚æ„ŸçŸ¥æœºæˆ–è€…å…¶ä»–çš„éå¾ªç¯çš„ç¥ç»ç½‘ç»œæ˜¯éš¾ä»¥å­¦ä¹ çš„**


-----


### 6. LSTM è¿”å› çŠ¶æ€è¿˜æ˜¯è¿”å›åºåˆ—


**return sequences** ï¼š å¦‚æœè¦è¿”å›åºåˆ—

```python
lstm1 = LSTM(1, return_sequences=True)
```

åˆ™ç½‘ç»œçš„æ¯ä¸€ä¸ªéšè—çŠ¶æ€(hidden state)å€¼éƒ½è¾“å‡º ï¼ˆç»´åº¦å¯¹åº”äºæ—¶é—´æ­¥ï¼‰ï¼Œ


![](https://github.com/LiuChuang0059/large_file/blob/master/pic/6dt8y.jpg)

å¦‚æœæ˜¯å †å ï¼ˆstacked LSTMï¼‰LSTM ç½‘ç»œï¼Œä¸€å®šè¦è¿”å›æ‰€æœ‰çŠ¶æ€å€¼ï¼Œä½œä¸ºä¸‹ä¸€å±‚ç½‘ç»œçš„è¾“å…¥

![](https://github.com/LiuChuang0059/large_file/blob/master/pic/e3sx0.jpg)

å¦‚æœä¸é€‰æ‹©è¿”å›çŠ¶æ€,åˆ™åªè¿”å›æœ€åçš„ä¸€ä¸ªçŠ¶æ€å€¼ --- å¸¸ç”¨äºå¼‚æ­¥çš„ seq2seq

![](https://github.com/LiuChuang0059/large_file/blob/master/pic/ua8wq.jpg)


-----


**return states**

```python
lstm1, state_h, state_c = LSTM(1, return_state=True)
```
lstmq1 : The LSTM hidden state output for the last time step.
state_h : The LSTM hidden state output for the last time step (again).
state_c : The LSTM cell state for the last time step.

ä¸åŒäºä¸Šæ–‡çš„éšè—çŠ¶æ€ hidden state , è¿™é‡Œçš„ State æŒ‡çš„æ˜¯æ¯ä¸ª LSTM cell çš„ **cell state**
> è¯¦è§ æœ¬æ–‡ç¬¬ä¸‰éƒ¨åˆ† LSTM è®°å¿†æœºåˆ¶

é€šå¸¸æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬ä¸éœ€è¦è·å– cell stateï¼Œä½†æ˜¯å¦‚æœæ˜¯ä¸€ä¸ªå¤æ‚çš„æ¨¡å‹ï¼Œåç»­çš„å±‚éœ€è¦å‰é¢å±‚çš„æœ€ç»ˆ cell state æ¥åˆå§‹åŒ–ä»–ä»¬çš„ cell stateã€‚
> ä¾‹å¦‚ encoder-decoder æ¨¡å‹

-----

åŒæ—¶ **Return States and Sequences**
```python
lstm1, state_h, state_c = LSTM(1, return_sequences=True, return_state=True)
```

**ä¸åŒäºä¸Šé¢çš„è¾“å‡º**
lstm1 :  returns the hidden state for each input time step
state_h: the hidden state output for the last time step
state_c : and the cell state for the last input time step.


-----

### 7. TimeDistributed layer
> This wrapper allows us to apply a layer to every temporal slice of an input
> TimeDistributedDense applies a same Dense (fully-connected) operation to every timestep of a 3D tensor.

#### 1. One-to-One LSTM for Sequence Prediction
> è¾“å…¥ sequence [0.0, 0.2, 0.4, 0.6, 0.8] ï¼› è¾“å‡º sequence [0.0, 0.2, 0.4, 0.6, 0.8]ï¼›ä¸€ä¸ªæ—¶é—´æ­¥è¾“å‡ºä¸€ä¸ªå€¼ã€‚

å®šä¹‰ç½‘ç»œæ¨¡å‹ï¼š ä¸€ä¸ªæ—¶é—´æ­¥ä¸€ä¸ªè¾“å…¥ï¼Œéšè—å±‚å°±æœ‰ 5ä¸ª units ï¼Œè¾“å‡ºå±‚æ˜¯ä¸€ä¸ªå…¨è¿æ¥å±‚å¯¹åº”ä¸€ä¸ªè¾“å‡º
> å…¨è¿æ¥å±‚ï¼š keras ä¸­çš„ Denseï¼š output = activation(dot(input, kernel) + bias)

```python

X = seq.reshape(len(seq), 1, 1) # 5 ä¸ªæ ·æœ¬ ä¸€ä¸ªæ—¶é—´æ­¥ ä¸€ä¸ªç‰¹å¾
y = seq.reshape(len(seq), 1) # 5ä¸ªæ ·æœ¬ 1ä¸ªç‰¹å¾

length = 5
n_neurons = length
# create LSTM
model = Sequential()
model.add(LSTM(n_neurons, input_shape=(1, 1)))
model.add(Dense(1))
model.compile(loss='mean_squared_error', optimizer='adam')
print(model.summary())

# train LSTM
model.fit(X, y, epochs=n_epoch, batch_size=n_batch, verbose=2)


```

å…¨è¿æ¥å±‚å‚æ•°è®¡ç®—ï¼š  5ä¸ª è¾“å…¥ï¼ˆæ¥è‡ªä¸Šä¸€å±‚ï¼‰ï¼Œ  ä¸€ä¸ªè¾“å‡ºï¼ˆä¸€ä¸ªç¥ç»å…ƒï¼‰ + bias
```python
n = inputs * outputs + outputs
n = 5 * 1 + 1
n = 6

```

-----

#### 2. Many-to-One LSTM for. Sequence Prediction

æ¨¡å‹ä¸€æ¬¡æ€§è¾“å‡ºåºåˆ— ï¼š è¾“å‡ºä¸€ä¸ªå‘é‡ï¼ˆ5ä¸ªæ—¶é—´æ­¥è¾“å‡ºä¸€ä¸ªï¼‰ï¼Œè€Œä¸æ˜¯ä¸€ä¸ªåºåˆ—ï¼ˆæ¯ä¸ªæ—¶é—´æ­¥è¾“å‡ºä¸€ä¸ªï¼‰

```python
X = seq.reshape(1, 5, 1) # ä¸€ä¸ªæ ·æœ¬ï¼Œ5ä¸ªæ—¶é—´æ­¥ï¼Œ ä¸€ä¸ªç‰¹å¾
y = seq.reshape(1, 5) #  ä¸€ä¸ªæ ·æœ¬ 5ä¸ªç‰¹å¾

# create LSTM
length = 5
model = Sequential()
model.add(LSTM(5, input_shape=(5, 1)))
model.add(Dense(length))  # 5 ä¸ªç¥ç»å…ƒçš„å…¨è¿æ¥å±‚
model.compile(loss='mean_squared_error', optimizer='adam')
print(model.summary())
```
å…¨è¿æ¥å±‚å‚æ•°è®¡ç®—ï¼š  5ä¸ª è¾“å…¥ï¼ˆæ¥è‡ªä¸Šä¸€å±‚ï¼‰ï¼Œ  5ä¸ªè¾“å‡ºï¼ˆ5ä¸ªç¥ç»å…ƒï¼‰ + bias


åºåˆ—ä½œä¸ºä¸€ä¸ªç‰‡æ®µè¢«ç”Ÿæˆï¼Œè€Œä¸æ˜¯é€æ­¥çš„è¾“å…¥æ•°æ®ï¼› ä½†æ˜¯è¿™æ ·æ²¡æœ‰åˆ©ç”¨åˆ° LSTMçš„ åºåˆ—å­¦ä¹ èƒ½åŠ›ã€‚

-----

#### 3. Many-to-Many LSTM for Sequence Prediction (with TimeDistributed)

ä½¿ç”¨ TimeDistributed çš„ è¦ç‚¹ï¼š

* è¾“å…¥å¿…é¡»æ˜¯ 3Dï¼› éœ€è¦é…ç½® TimeDistributed dense å±‚çš„å‰ä¸€å±‚ **è¿”å›åºåˆ—**
> you will need to configure your last LSTM layer prior to your TimeDistributed wrapped Dense layer to return sequences eg: return_sequences = True

* è¾“å‡ºå¯èƒ½æ˜¯ 3D ï¼š å¦‚æœ TimeDistributed dense å±‚ æ˜¯è¾“å‡ºå±‚ï¼Œè¾“å‡ºä¸€ä¸ªåºåˆ—ï¼Œå¯èƒ½éœ€è¦å°† y reshape into 3D



æ¯ä¸ª LSTM unit è¿”å›ä¸€ä¸ª åŒ…å«äº”ä¸ªè¾“å‡ºå€¼çš„åºåˆ—ï¼Œæ¯ä¸ªè¾“å‡ºå¯¹åº”è¾“å…¥æ•°æ®çš„ä¸€ä¸ªæ—¶é—´æ­¥
```python
model.add(LSTM(n_neurons, input_shape=(length, 1), return_sequences=True))
```


ä½¿ç”¨  TimeDistributed  åŒ…è£¹ å•ä¸ªè¾“å‡ºçš„çš„ å…¨è¿æ¥ dense å±‚

```python
model.add(TimeDistributed(Dense(1)))
```

**è¾“å‡ºå±‚çš„å•å€¼è¾“å‡ºæ˜¯å…³é”®**ï¼š å¯¹äºè¾“å…¥åºåˆ—çš„æ—¶é—´æ­¥ï¼Œæˆ‘ä»¬æ¯æ¬¡åªè¾“å‡ºæ•°åˆ—ä¸­çš„ä¸€æ­¥çš„å€¼ï¼Œ

TimeDistributed å¯¹äºæ¯ä¸ªæ—¶é—´æ­¥çš„ LSTM è¾“å‡ºéƒ½åº”ç”¨ä¸€æ¬¡ Denseå±‚çš„ è¿ç®— ï¼ˆ5ä¸ªè¾“å…¥ä¸€ä¸ªè¾“å‡º  + bias æ±‡æˆ ä¸€ä¸ªåºåˆ—ï¼‰
> The TimeDistributed achieves this trick by applying the same Dense layer (same weights) to the LSTMs outputs for one time step at a time. In this way, the output layer only needs one connection to each LSTM unit (plus one bias).


å…¨è¿æ¥å±‚å‚æ•°è®¡ç®—ï¼š  5ä¸ª è¾“å…¥ï¼ˆæ¥è‡ªä¸Šä¸€å±‚ï¼‰ï¼Œ  ä¸€ä¸ªè¾“å‡ºï¼ˆä¸€ä¸ªç¥ç»å…ƒæ¯ä¸ªè¾“å…¥æœ‰ä¸€ä¸ªæƒé‡ï¼‰ + bias


----

### 8. Dense and TimeDistributedDense of Keras çš„åŒºåˆ«

1. å‡è®¾æœ‰ N ç»„æ•°æ®ï¼Œé­…æ—æ•°æ® 700 ä¸ªç‰¹å¾ï¼ˆ700 ä¸ªå€¼ï¼‰ï¼Œå¾—åˆ°ä¸€ä¸ª 200ä¸ªå€¼è¾“å‡ºã€‚é¦–å…ˆéœ€è¦å°†data reshape ï¼ˆNï¼Œ700ï¼Œ1ï¼‰

2. 700 å°±æ˜¯æ—¶é—´æ­¥ï¼Œ æ•°æ®ä» t = 1 åˆ° 700ï¼Œè¾“å…¥åˆ°RNNï¼Œ å¯¹åº” 700 ä¸ªè¾“å‡ºï¼Œå°±æ˜¯ h1 åˆ° h700. æ‰€ä»¥è¾“å‡ºçš„æ•°æ®æ ¼å¼ N * 700* 200(channels)
![](https://github.com/LiuChuang0059/large_file/blob/master/pic/g34gb.jpg)

3. ä½¿ç”¨ TimeDistributedDensï¼Œ ç›¸å½“äºå¯¹äºæ¯ä¸€ä¸ªæ—¶é—´æ­¥ï¼Œéƒ½ç”¨äº†ä¸€ä¸ª denseå±‚ï¼Œå°±æ˜¯ å¯¹ h1ï¼Œh2,h3 åˆ†åˆ«ç”¨ dense å±‚ã€‚ ä» h1 åˆ° h700 ï¼Œå¯¹ä»–ä»¬çš„ channels åˆ†åˆ«ä½¿ç”¨å…¨è¿æ¥æ“ä½œ --- å˜ä¸º ï¼ˆ1ï¼Œ1ï¼Œ200ï¼‰

4. ä¸ºä»€ä¹ˆè¿™ä¹ˆåšï¼Ÿ -- ä¸å¸Œæœ› Flattern RNN çš„è¾“å‡º

5. ä¸ºä»€ä¸ flattern --- å¸Œæœ›æ—¶é—´æ­¥å€¼ç‹¬ç«‹

6. ä¸ºä»€ä¹ˆç‹¬ç«‹ï¼Ÿ

* åªæƒ³åœ¨è‡ªå·±çš„æ—¶é—´æ­¥æœ‰äº¤äº’

* ä¸æƒ³ä¸åŒçš„æ—¶é—´æ­¥å’Œ channels æœ‰äº¤äº’ã€‚


----

### 9. When and How to use TimeDistributedDense

![](https://github.com/LiuChuang0059/large_file/blob/master/pic/i37fp.jpg)

RNN å¯ä»¥å®ç°ä¸åŒç±»å‹çš„è½¬æ¢ï¼ŒTimeDistributedDense å…è®¸æˆ‘ä»¬æ„å»º _one-to-many_ and _many-to-many_ ç»“æ„çš„æ¨¡å‹ã€‚å› ä¸ºå¯¹äºå¤šä¸ªè¾“å‡ºçš„è¾“å‡ºå‡½æ•°ï¼Œåœ¨æ¯ä¸ªæ—¶é—´æ­¥ä¸Šå¿…é¡»ç›¸åŒã€‚

å¦‚æœä¸ä½¿ç”¨è¿™ä¸ªï¼Œåªèƒ½æœ‰ä¸€ä¸ªè¾“å‡ºï¼Œåœ¨æœ€åä½¿ç”¨ä¸€ä¸ª demnse layer


> For each sample, the input is a sequence (a1,a2,a3,a4...aN) and the output is a sequence (b1,b2,b3,b4...bN) with the same length. bi could be viewed as the label of ai.
Push a1 into a recurrent nn to get output b1. Than push a2 and the hidden output of a1 to get b2...

ä½¿ç”¨ TimeDistributedDense ï¼Œè®¡ç®—å‡ºæ¥çš„ æŸå¤±å‡½æ•°æ˜¯æ‰€æœ‰æ—¶é—´æ­¥ä¸Šé¢çš„è¾“å‡ºï¼Œä¸ä½¿ç”¨çš„è¯ï¼ŒæŸå¤±éŸ©å¼åªæ˜¯æœ€åä¸€æ­¥çš„è¾“å‡ºï¼Œåªèƒ½ç­‰åˆ°æœ€åçš„ bN

----



### 10. é¿å… LSTM æ¨¡å‹çš„ Overfitting and Underfitting


#### 1. good fit

![](https://github.com/LiuChuang0059/large_file/blob/master/pic/qkgft.jpg)

#### 2. Underfit

* åœ¨è®­ç»ƒé›†ä¸Šè¡¨ç°å¥½ï¼Œåœ¨ test set ä¸Šè¡¨ç°çš„æ¯”è¾ƒå·®ï¼Œ validation loss æœ‰æé«˜æ›´å¥½çš„å¯èƒ½
> åŠ è®­ç»ƒçš„ epochs

![](https://github.com/LiuChuang0059/large_file/blob/master/pic/52ye8.jpg)


* å¦ä¸€ç§æ˜¯ è®­ç»ƒé›†æ¯” validation set å¥½å¾ˆå¤šï¼Œè€Œä¸”è®­ç»ƒå·²ç»è¶‹äºç¨³å®š
> è¿™ç§æƒ…å†µéœ€è¦æé«˜æ¨¡å‹çš„èƒ½åŠ›ï¼Œ units ä¸ªæ•°ï¼Œ éšè—å±‚çš„æ•°ç›®

![](https://github.com/LiuChuang0059/large_file/blob/master/pic/a2ngq.jpg)


#### 3. Overfit

* validation set ç²¾åº¦åœ¨æŸä¸ªç‚¹å¼€å§‹åå¼¹
> åœ¨åå¼¹ åœ°æ–¹åœæ­¢epochï¼Œ æˆ–è€…å¢åŠ  æ•°æ®é›†ã€‚

![](https://github.com/LiuChuang0059/large_file/blob/master/pic/03kqz.jpg)



#### 4.  å¤šæ¬¡è¿è¡Œ

**LSTM æ˜¯éšæœºçš„**ï¼Œæ¯ run ä¸€æ¬¡å¾—åˆ°ä¸€ä¸ªä¸åŒçš„å›¾

é‡å¤ 5 åˆ° 10æ¬¡ run ï¼Œtrain and validation çš„å¤šæ¬¡ plot èƒ½å¤Ÿç»™å‡ºä¸€ä¸ªæ›´é²æ£’çš„æ¨¡å‹è¡¨ç°æƒ…å†µ

```python
for i in range(5):
	# define model
	model = Sequential()
	model.add(LSTM(10, input_shape=(1,1)))
	model.add(Dense(1, activation='linear'))
	# compile model
	model.compile(loss='mse', optimizer='adam')
	X,y = get_train()
	valX, valY = get_val()
	# fit model
	history = model.fit(X, y, epochs=300, validation_data=(valX, valY), shuffle=False)
	# story history
	train[str(i)] = history.history['loss']
	val[str(i)] = history.history['val_loss']

# plot train and validation loss across multiple runs
pyplot.plot(train, color='blue', label='train')
pyplot.plot(val, color='orange', label='validation')
pyplot.title('model train vs validation loss')
pyplot.ylabel('loss')
pyplot.xlabel('epoch')
pyplot.show()


```

-----

### 11.  LSTM è¾“å…¥æ•°æ®æ ¼å¼


```python

model = Sequential()
model.add(LSTM(32,input_shape=(50,2)))
model.add(Dense(1))
```

LSTM è¾“å…¥æ˜¯ä¸‰ç»´çš„

* æ ·å“ ï¼š ä¸€ä¸ªåºåˆ—æ˜¯ä¸€ä¸ªæ ·æœ¬ã€‚æ‰¹æ¬¡ç”±ä¸€ä¸ªæˆ–å¤šä¸ªæ ·æœ¬ç»„æˆã€‚

* æ—¶é—´æ­¥ ï¼š ä¸€ä¸ªæ—¶é—´æ­¥ä»£è¡¨æ ·æœ¬ä¸­çš„ä¸€ä¸ªè§‚å¯Ÿç‚¹ã€‚ ï¼ˆ50ä¸ªæ—¶é—´æ­¥é•¿ï¼Œ2ä¸ªç‰¹å¾ï¼‰

* ç‰¹å¾ ï¼š ä¸€ä¸ªç‰¹å¾æ˜¯åœ¨ä¸€ä¸ªæ—¶é—´æ­¥é•¿çš„è§‚å¯Ÿå¾—åˆ°çš„ã€‚



```python
data= data.reshape(1,10,1)  # æ¯ä¸ªæ—¶é—´æ­¥é•¿ä¸€ä¸ªæ ·æœ¬ éœ€è¦10ä¸ªæ—¶é—´æ­¥é•¿å’Œ1ä¸ªç‰¹å¾ã€‚
model.add(LSTM(32,input_shape(10,1)))
```
è¯¥æ•°æ®ç°åœ¨å¯ä»¥ä¸ºinput_shapeï¼ˆ10ï¼Œ1ï¼‰çš„LSTMçš„è¾“å…¥ï¼ˆXï¼‰ã€‚



----

**LSTM æœ‰å¤šä¸ªè¾“å…¥ç‰¹å¾**

ä¾‹å¦‚ï¼š æœ‰ä¸¤ä¸ªå¹³è¡Œåºåˆ—ä½œä¸ºè¾“å…¥

```python
series 1: 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0
series 2: 1.0, 0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1

```

æ•°æ® ï¼š ä¸€ä¸ª æ ·æœ¬ï¼› 10ä¸ª æ—¶é—´æ­¥ï¼Œ 2ä¸ªç‰¹å¾

```python
data = data.reshape(1, 10, 2)

```

---

LSTM è¾“å…¥ Tips

* LSTM è¾“å…¥å±‚å¿…é¡»æ˜¯ 3D

* LSTM è¾“å…¥å±‚æ˜¯ç”± ç¬¬ä¸€éšè—å±‚ çš„ input_shape å®šä¹‰

* input_shape è¾“å…¥ä¸€ä¸ª tuple ï¼ˆæ—¶é—´æ­¥ï¼Œç‰¹å¾æ•°ï¼‰


-----

### 12.  è®­ç»ƒä¸€ä¸ª final LSTM æ¨¡å‹
final LSTM æ¨¡å‹ ï¼š ç”¨äºé¢„æµ‹æ–°çš„æ•°æ®çš„æ¨¡å‹

#### 1. å¦‚ä½•æœ€ç»ˆåŒ–ä¸€ä¸ª LSTM æ¨¡å‹ -- æ‰€æœ‰æ•°æ®

ç”¨æ‰€æœ‰çš„æ•°æ®æ¥ fit æ¨¡å‹ï¼ˆæ²¡æœ‰ test set ï¼Œvalidationï¼‰

* æœ€ç»ˆæ¨¡å‹å¯ä»¥ ä¿å­˜ï¼Œ
* å¯¼å…¥æ¨¡å‹ç”¨æ¥æ–°æ•°æ®çš„é¢„æµ‹

#### 2. Train/Test set çš„ç›®çš„

åˆ†å‰²æ•°æ®é›†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†å¯ä»¥å¿«é€Ÿçš„è¯„ä¼°æ¨¡å‹çš„è¡¨ç°

æˆ‘ä»¬å‡è®¾æµ‹è¯•é›†æ•°æ®æ˜¯æ–°çš„æ•°æ®ï¼Œæ¯”è¾ƒé¢„æµ‹çš„å€¼å’ŒçœŸå®çš„å€¼ï¼Œèƒ½å¤Ÿè¯„ä¼°æ¨¡å‹çš„è¡¨ç°æƒ…å†µ

**æˆ‘ä»¬æ ¹æ®æ¨¡å‹åœ¨ test set ä¸Šçš„è¡¨ç° æ¨æ–­æ¨¡å‹åœ¨æ–°çš„æ•°æ®ä¸Šçš„è¡¨ç°**ï¼Œæ˜¯ä¸€ä¸ªå¾ˆå¤§çš„è·³è·ƒï¼Œéœ€è¦å¾ˆå¤šçš„çº¦æŸ

* è¿‡ç¨‹è¦ä¸¥æ ¼é²æ£’

* æ¨¡å‹è¡¨ç°æµ‹é‡çš„é€‰æ‹©è¦èƒ½å¤ŸåŒ…æ‹¬ä½ç½®æ•°æ®çš„æ„Ÿå…´è¶£çš„ç‰¹å¾

* æ•°æ®é¢„å¤„ç†åœ¨æ–°æ•°æ®ä¸Šå¯é‡å¤ï¼Œæ•°æ®å¯é€†

* é€‰æ‹©çš„ç®—æ³•å¯¹å®éªŒçš„é—®é¢˜æ˜¯æœ‰æ„ä¹‰çš„


å®é™…ä¸Šï¼Œä½¿ç”¨test set åˆ¤æ–­ unseen data é€šå¸¸æœ‰ä¸€å®šçš„åå·®ï¼Œé™¤éæœ‰å¤§é‡çš„æ•°æ®ã€‚é‡å¤å¤šæ¬¡å®éªŒï¼Œé€šå¸¸å¾—åˆ°å¤šä¸ªç»“æœã€‚è¿™æ ·å¯èƒ½ä¼šå¯¼è‡´æˆ‘ä»¬ä¸ç¡®å®šç¨‹åºåœ¨ unseen data ä¸Šçš„å®é™…è¡¨ç°ã€‚ é€šå¸¸æƒ…å†µä¸‹å¦‚æœæ—¶é—´å…è®¸ï¼Œæˆ‘ä»¬æ›´å€¾å‘äºä½¿ç”¨ **k-fold cross-validation**


#### 3. ä½¿ç”¨ k-fold cross-validation çš„ç›®çš„

cross-validation èƒ½å¤Ÿç³»ç»Ÿçš„åˆ›å»ºå’Œè¯„ä¼°å¤šä¸ªå­æ•°æ®é›†ä¸Šçš„å¤šä¸ªæ¨¡å‹çš„æ•ˆæœ

* è®¡ç®—å¤šä¸ªæ¨¡å‹çš„å¹³å‡å€¼è¯„ä¼°æ¨¡å‹çš„å¹³å‡è¡¨ç°æƒ…å†µ

* è®¡ç®—æµ‹é‡æ–¹æ³•çš„æ ‡å‡†è¯¯å·®ï¼Œ ä»è€Œäº†è§£å®é™…æƒ…å†µä¸‹çš„åå·®

#### 4. é‡é‡‡æ ·çš„æ–¹æ³•

Both train-test splits and k-fold cross validation éƒ½æ˜¯é‡é‡‡æ ·æ–¹æ³•

**Whyï¼Ÿ** æˆ‘ä»¬æƒ³è¦è®­ç»ƒçš„ç†æƒ³æ¨¡å‹å°±æ˜¯èƒ½å¤Ÿåœ¨æ–°æ•°æ®è¡¨ç°çš„æœ€å¥½ï¼Œä½†æ˜¯æˆ‘ä»¬æ²¡æœ‰æ–°æ•°æ®ï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦ä½¿ç”¨ç»Ÿè®¡æŠ€å·§é¢„æµ‹ã€‚


#### 5. é—®é¢˜

1. ä¸ºä»€ä¹ˆä¸ä½¿ç”¨cross-validation ä¸­æœ€å¥½çš„æ¨¡å‹ï¼Œè€Œæ˜¯ä½¿ç”¨ final model
> final model ä½¿ç”¨çš„æ•°æ®æ˜¯æ‰€æœ‰å¯ç”¨æ•°æ®ï¼Œï¼Œè€Œ å…¶ä»–çš„åªæ˜¯ä½¿ç”¨çš„å­æ•°æ®é›†


2. åœ¨æ•´ä¸ªæ•°æ®é›†ä¸Šé¢è®­ç»ƒçš„æ¨¡å‹ä¼šæœ‰ä¸åŒå—ï¼Œæœ‰ä»€ä¹ˆä¸åŒ ---- è®­ç»ƒåœ¨æ•´ä¸ªæ•°æ®é›†ä¸Šé¢ï¼Œæ€ä¹ˆçŸ¥é“æ¨¡å‹çš„å¥½å
> If well designed, the performance measures you calculate using train-test or k-fold cross validation suitably describe how well the finalized model trained on all available historical data will perform in general.

3. æ¯æ¬¡è®­ç»ƒçš„æ¨¡å‹éƒ½å¾—åˆ°ä¸åŒçš„è¡¨ç°ç»“æœï¼Ÿ
> æœºå™¨å­¦ä¹ ç®—æ³•æ˜¯æœ‰éšæœºæ€§çš„ï¼Œå‡ºç°ä¸åŒçš„ç»“æœå¾ˆæ­£å¸¸ã€‚å¤šæ¬¡é‡å¤å®éªŒ train/test ä¼šå¸®åŠ©æˆ‘ä»¬æŒæ¡å˜åŒ–çš„èŒƒå›´


-----


### 13.  ä¿å­˜ final model

keras ä¸­ æ¨¡å‹å¯ä»¥ä¿å­˜åœ¨ HDF5 ç±»å‹æ–‡ä»¶ä¸­,å°†ä¿å­˜æ¨¡å‹çš„æ¡†æ¶ç»“æ„å’Œæƒé‡ï¼Œä»¥åŠæ¨¡å‹é€‰å–çš„ æŸå¤±å‡½æ•°ï¼Œä¼˜åŒ–ç®—æ³•ã€‚
æ¨¡å‹çš„ç½‘ç»œç»“æ„ä¿å­˜åœ¨ JSON æˆ–è€… YAML æ–‡ä»¶é‡Œé¢ã€‚ï¼ˆè¯¦è§å¦ä¸€ç¯‡ç¬”è®° ï¼‰

```python
# save model to single file
model.save('lstm_model.h5')

```


æ¨¡å‹å¯ä»¥å†æ¬¡**å¯¼å…¥**
> å¯èƒ½æ˜¯å†æ¬¡æ‰“å¼€æ–‡ä»¶ï¼Œæˆ–è€…åœ¨å¦ä¸€ä¸ª script ä¸­æ‰“å¼€

```python
from keras.models import load_model
# load model from single file
model = load_model('lstm_model.h5')
# make predictions
yhat = model.predict(X, verbose=0)
```

----

### 14. LSTM å‚æ•°è®¡ç®—

å‚è€ƒå¦ä¸€ç¯‡ç¬”è®° https://github.com/LiuChuang0059/ComplexNetwork-DataMining/blob/master/techs/RNN/LSTM%E5%8F%82%E6%95%B0%E8%AE%A1%E7%AE%97.md

---

### 15. LSTM ä¸­ Units å‚æ•° å’Œ cell

![](https://github.com/LiuChuang0059/large_file/blob/master/pic/c1q1u.jpg)

cell é‡Œé¢æœ‰å››ä¸ªé»„è‰²å°æ¡†ï¼Œæ¯ä¸€ä¸ªå°é»„æ¡†ä»£è¡¨ä¸€ä¸ªå‰é¦ˆç½‘ç»œå±‚ï¼Œnum_units å°±æ˜¯è¿™ä¸ªå±‚çš„éšè—ç¥ç»å…ƒä¸ªæ•°ï¼Œå…¶ä¸­1ã€2ã€4çš„æ¿€æ´»å‡½æ•°æ˜¯ sigmoidï¼Œç¬¬ä¸‰ä¸ªçš„æ¿€æ´»å‡½æ•°æ˜¯ tanhã€‚

![](https://github.com/LiuChuang0059/large_file/blob/master/pic/oqgu4.jpg)

Num_units æŒ‡çš„æ˜¯ä¸€å±‚çš„output size(hidden size)

LSTM cell ä¼šå°†æ¯ä¸€æ—¶åˆ»çš„ x dim éçº¿æ€§å˜åŒ–ä¸º output hidden size



---

### 16. LSTM æ¨¡å‹é¢„æµ‹è‚¡ç¥¨ ---coding

https://github.com/LiuChuang0059/ComplexNetwork-DataMining/blob/master/techs/RNN/LSTM-keras.ipynb




-----


### 17. How do LSTMs Work?

 ç”±äºä¹‹å‰çš„ç½‘ç»œä¸­åå‘ä¼ æ’­ä¸­çš„é”™è¯¯è¦ä¹ˆæ˜¯æŒ‡æ•°å½¢å¼æ¿€å¢è¦ä¹ˆæ˜¯æŒ‡æ•°å½¢å¼è¡°å‡ï¼Œæ— æ³•è·å–é•¿æœŸè®°å¿†ã€‚
è€Œ LSTM å±‚åŒ…å«äº†ä¸€ç»„å¾ªç¯è¿æ¥çš„å—ï¼Œç§°ä¸ºè®°å¿†å—ã€‚è¿™äº›è®°å¿†å—å¯ä»¥è¢«çœ‹ä½œæ•°å­—ç”µè„‘ä¸­çš„è®°å¿†èŠ¯ç‰‡ã€‚æ¯ä¸ªå•å…ƒåŒ…å«ä¸€ä¸ªæˆ–å¤šä¸ªå¾ªç¯è¿æ¥çš„è®°å¿†å•å…ƒå’Œä¸‰ä¸ªä¹˜æ³•å•å…ƒâ€”â€”è¾“å…¥ã€è¾“å‡ºå’Œé—å¿˜é—¨â€”â€”å®ƒä»¬ä¸ºå•å…ƒæä¾›è¿ç»­çš„å†™ã€è¯»å’Œé‡ç½®æ“ä½œçš„æ¨¡æ‹Ÿã€‚ç½‘ç»œåªèƒ½é€šè¿‡é—¨ä¸ç»†èƒç›¸äº’ä½œç”¨ã€‚

> The Long Short Term Memory architecture was motivated by an analysis of error flow in existing RNNs which found that long time lags were inaccessible to existing architectures, because backpropagated error either blows up or decays exponentially.

An LSTM layer consists of a set of recurrently connected blocks, known as memory blocks. These blocks can be thought of as a differentiable version of the memory chips in a digital computer. Each one contains one or more recurrently connected memory cells and three multiplicative units â€“ the input, output and forget gates â€“ that provide continuous analogues of write, read and reset operations for the cells. â€¦ The net can only interact with the cells via the gates.  ---Alex Graves, et al., Framewise Phoneme Classification with Bidirectional LSTM and Other Neural Network Architectures, 2005.



---



### 18. What are Bidirectional LSTMs?


åŒå‘é€’å½’ç½‘ç»œçš„åŸºæœ¬æ€æƒ³å°±æ˜¯ï¼Œå°†æ¯ä¸ªè®­ç»ƒåºåˆ—å‘å‰å’Œå‘åè¡¨ç¤ºä¸ºä¸¤ä¸ªç‹¬ç«‹çš„é€’å½’ç½‘ç»œï¼Œå®ƒä»¬éƒ½è¿æ¥åˆ°åŒä¸€ä¸ªè¾“å‡ºå±‚ã€‚è¿™æ„å‘³ç€å¯¹äºç»™å®šåºåˆ—ä¸­çš„æ¯ä¸ªç‚¹ï¼ŒBRNNéƒ½æœ‰å…³äºå®ƒä¹‹å‰å’Œä¹‹åçš„æ‰€æœ‰ç‚¹çš„å®Œæ•´çš„é¡ºåºä¿¡æ¯

å¯¹äºåƒè¯­éŸ³è¯†åˆ«è¿™æ ·çš„æš‚æ—¶æ€§é—®é¢˜ï¼Œä¾èµ–äºå¯¹æœªæ¥çš„è®¤çŸ¥ä¹ä¸€çœ‹ä¼¼ä¹è¿åäº†å› æœå…³ç³»ï¼Œæˆ‘ä»¬å¦‚ä½•æ‰èƒ½å°†æˆ‘ä»¬å¯¹æ‰€å¬åˆ°çš„ä¸œè¥¿çš„ç†è§£å»ºç«‹åœ¨å°šæœªè¯´è¿‡çš„ä¸œè¥¿ä¹‹ä¸Šå‘¢?

ç„¶è€Œï¼Œ äººç±»å¬ä¼—æ­£æ˜¯è¿™æ ·åšçš„ã€‚å¬åˆ°çš„å£°éŸ³ï¼Œå•è¯ï¼Œé˜µé˜µæ•´å¥è¯ï¼Œå¯èƒ½åˆšå¼€å§‹å¹¶æ²¡æœ‰ä»€ä¹ˆæ„ä¹‰ï¼Œåªæœ‰å¬åˆ°åæ¥çš„è¯ æ‰æ˜ç™½æ„ä¹‰
> The basic idea of bidirectional recurrent neural nets is to present each training sequence forwards and backwards to two separate recurrent nets, both of which are connected to the same output layer. â€¦ This means that for every point in a given sequence, the BRNN has complete, sequential information about all points before and after it. Also, because the net is free to use as much or as little of this context as necessary, there is no need to find a (task-dependent) time-window or target delay size.

â€¦ for temporal problems like speech recognition, relying on knowledge of the future seems at first sight to violate causality â€¦ How can we base our understanding of what weâ€™ve heard on something that hasnâ€™t been said yet? However, human listeners do exactly that. Sounds, words, and even whole sentences that at first mean nothing are found to make sense in the light of future context.


![](https://github.com/LiuChuang0059/large_file/blob/master/pic/9k44m.jpg)

ä¸¾ä¾‹æ¥è¯´ï¼Œâ€æˆ‘å–œæ¬¢è‹¹æœï¼Œå› ä¸ºå®ƒå¾ˆå¥½åƒâ€ï¼Ÿå’Œâ€æˆ‘å–œæ¬¢è‹¹æœï¼Œå› ä¸ºä»–æ¯”å®‰å“ç¨³å®šâ€è¿™ä¸¤ä¸ªå¥å­å½“ä¸­ï¼Œå¦‚æœåªçœ‹â€æˆ‘å–œæ¬¢è‹¹æœâ€ï¼Œä½ å¯èƒ½ä¸çŸ¥é“è‹¹æœæŒ‡çš„æ˜¯æ°´æœè¿˜æ˜¯æ‰‹æœºï¼Œä½†å¦‚æœå¯ä»¥æ ¹æ®åé¢é‚£å¥å¾—åˆ°ä¿¡æ¯ï¼Œç­”æ¡ˆå°±å¾ˆæ˜¾è€Œæ˜“è§ï¼Œè¿™å°±æ˜¯åŒå‘RNNè¿ä½œçš„æ–¹å¼ã€‚




-----









----



# å‚è€ƒ

1. http://blog.echen.me/2017/05/30/exploring-lstms/

2. http://colah.github.io/posts/2015-08-Understanding-LSTMs/

3. Understand the Difference Between Return Sequences and Return States for LSTMs in Keras---
https://machinelearningmastery.com/return-sequences-and-return-states-for-lstms-in-keras/

4. How to Use the TimeDistributed Layer for Long Short-Term Memory Networks in Python --- https://machinelearningmastery.com/timedistributed-layer-for-long-short-term-memory-networks-in-python/

5. https://datascience.stackexchange.com/questions/10836/the-difference-between-dense-and-timedistributeddense-of-keras


6. When and How to use TimeDistributedDense -- https://github.com/keras-team/keras/issues/1029

7. How to Diagnose Overfitting and Underfitting of LSTM Models --- https://machinelearningmastery.com/category/lstm/page/2/

8. How to Reshape Input Data for Long Short-Term Memory Networks in Keras --https://machinelearningmastery.com/reshape-input-data-long-short-term-memory-networks-keras/

9. å¦‚ä½•ä¸ºLSTMé‡æ–°æ„å»ºè¾“å…¥æ•°æ®ï¼ˆKerasï¼‰--- https://www.jianshu.com/p/246f117af8f0

10. Data and Notebook for the Stock Price Prediction Tutorial-- https://github.com/mwitiderrick/stockprice


11. How to Make Predictions with Long Short-Term Memory Models in --- https://machinelearningmastery.com/make-predictions-long-short-term-memory-models-keras/


12. Alex Graves, et al., Framewise Phoneme Classification with Bidirectional LSTM and Other Neural Network Architectures, 2005.


13.  ğŸŒŸ A Gentle Introduction to Long Short-Term Memory Networks by the Experts---https://machinelearningmastery.com/gentle-introduction-long-short-term-memory-networks-experts/


14.  ğŸŒŸ Understanding LSTM Networks ---http://colah.github.io/posts/2015-08-Understanding-LSTMs/


15. åœ¨è°ƒç”¨APIä¹‹å‰ï¼Œä½ éœ€è¦ç†è§£çš„LSTMå·¥ä½œåŸç† --- https://cloud.tencent.com/developer/article/1120123




16. ä¸­æ–‡åˆ†è¯ç³»åˆ— åŸºäºåŒå‘LSTMçš„seq2seqå­—æ ‡æ³¨---- https://spaces.ac.cn/archives/3924


17. https://www.zhihu.com/question/64470274
